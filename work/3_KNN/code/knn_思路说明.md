# KNN实验思路说明

## 1. 算法原理
K最近邻（K-Nearest Neighbors, KNN）是一种基本的监督学习分类算法。对于待分类样本，计算其与训练集中所有样本的距离，选取距离最近的K个邻居，根据邻居的类别多数表决确定预测类别。

## 2. 实现要点
- **距离度量**：本实验采用欧氏距离（可扩展为曼哈顿等）。
- **K值选择**：K为超参数，需通过实验调优。K过小易受噪声影响，K过大则可能掩盖局部特征。
- **多数表决**：对K个最近邻的标签计数，选择出现次数最多的类别。
- **纯Python实现**：不依赖sklearn等外部KNN库，核心算法手写。

## 3. 参数调优
- 通过遍历不同K值（如1~20），在测试集上评估准确率，绘制K-准确率曲线，选择最优K。

## 4. 实验流程
1. 读取前面阶段标准化后的训练集和测试集（csv文件）。
2. 手写KNN分类器，支持K值调参。
3. 训练模型并在测试集上预测。
4. 评估模型性能，包括准确率、混淆矩阵等。
5. 输出K-准确率曲线、混淆矩阵等可视化结果。

## 5. 结果输出
- `k_acc_curve.csv`、`k_acc_curve.png`：K值与准确率关系
- `confusion_matrix.csv`、`confusion_matrix.png`：最优K下的混淆矩阵
- `best_k_acc.txt`：最优K和对应准确率

## 6. 可扩展性
- 可支持不同距离度量、加权KNN等
- 可输出更多评估指标（如精确率、召回率等）

---
本实验流程清晰、结果可复现，便于后续模型对比与优化。 