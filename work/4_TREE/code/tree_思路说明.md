# 决策树实验思路说明

## 1. 算法原理
决策树（Decision Tree）是一种常用的监督学习分类方法。其基本思想是通过特征的条件分裂，将样本空间递归划分为若干子空间，最终在叶节点给出类别预测。本实验采用基尼指数（Gini）作为分裂标准。

## 2. 实现要点
- **分裂标准**：采用基尼指数，选择能最大程度降低基尼指数的特征和阈值进行分裂。
- **递归生长**：通过递归方式构建树结构，直到节点纯净或达到最大深度。
- **预测方式**：从根节点出发，根据特征阈值递归下行，最终到达叶节点给出类别。
- **纯Python实现**：不依赖sklearn等外部库，核心算法手写。

## 3. 参数设置
- `max_depth`：树的最大深度，防止过拟合。通过批量实验，**推荐max_depth=2**，此时模型在鸢尾花数据集上表现最优。

## 4. 实验流程
1. 读取前面阶段标准化后的训练集和测试集（csv文件）。
2. 手写决策树分类器，支持最大深度调参。
3. 训练模型并在测试集上预测。
4. 评估模型性能，包括准确率、混淆矩阵、精确率、召回率、特异度等。
5. 输出混淆矩阵热图、评估指标条形图、三线表等可视化结果。
6. **批量调参**：自动遍历不同max_depth，输出每个深度下的准确率和评估指标，辅助选择最优参数。

## 5. 评估指标
- **准确率**：整体分类正确率
- **混淆矩阵**：各类别分类情况
- **精确率、召回率、特异度**：多角度评价模型性能，支持宏平均

## 6. 结果输出
- `accuracy.txt`：准确率
- `confusion_matrix.csv`、`confusion_matrix_heatmap.png`：混淆矩阵及热图
- `tree_metrics.csv`、`tree_metrics_bar.png`、`tree_metrics_table.png`：评估指标及可视化
- `depth_metrics.csv`、`depth_acc_curve.png`：不同max_depth下的对比结果

## 7. 结论与建议
- 批量实验表明，max_depth=2时决策树在鸢尾花数据集上性能最佳，准确率和各项指标均接近1。
- 深度过小（如1）欠拟合，过大（≥3）无明显提升甚至略有过拟合。
- 推荐后续对比分析均采用max_depth=2的决策树。

## 8. 可扩展性
- 可支持信息增益分裂、剪枝、特征重要性等
- 可输出更多评估指标（如F1分数等）

---
本实验流程规范，结果可复现，便于与KNN等模型对比分析。 